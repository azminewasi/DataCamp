{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course - Dimensionality Reduction in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual feature extraction I\n",
    "You want to compare prices for specific products between stores. The features in the pre-loaded dataset sales_df are: storeID, product, quantity and revenue. The quantity and revenue features tell you how many items of a particular product were sold in a store and what the total revenue was. For the purpose of your analysis it's more interesting to know the average price per product.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Calculate the product price from the quantity sold and total revenue.\n",
    "Drop the quantity and revenue features from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the price from the quantity sold and revenue\n",
    "sales_df['price'] = sales_df['revenue'] / sales_df['quantity']\n",
    "\n",
    "# Drop the quantity and revenue features\n",
    "reduced_df = sales_df.drop(['revenue','quantity'], axis=1)\n",
    "\n",
    "print(reduced_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual feature extraction II\n",
    "You're working on a variant of the ANSUR dataset, height_df, where a person's height was measured 3 times: height_1, height_2, height_3. Add a feature with the mean height to the dataset, then drop the 3 original features.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Add a feature with the mean height to the dataset. Use the .mean() method with axis=1.\n",
    "Drop the 3 original height features from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean height\n",
    "height_df['height'] = height_df[[\"height_1\", \"height_2\", \"height_3\"]].mean(axis=1)\n",
    "\n",
    "# Drop the 3 original height features\n",
    "reduced_df = height_df.drop([\"height_1\", \"height_2\", \"height_3\"], axis=1)\n",
    "\n",
    "print(reduced_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Principal Components\n",
    "You'll visually inspect a 4 feature sample of the ANSUR dataset before and after PCA using Seaborn's pairplot(). This will allow you to inspect the pairwise correlations between the features.\n",
    "\n",
    "The data has been pre-loaded for you as ansur_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Create the PCA instance and fit and transform the data with pca\n",
    "pca = PCA()\n",
    "pc = pca.fit_transform(ansur_std)\n",
    "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
    "\n",
    "# Create a pairplot of the principal component DataFrame\n",
    "sns.pairplot(pc_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on a larger dataset\n",
    "You'll now apply PCA on a somewhat larger ANSUR datasample with 13 dimensions, once again pre-loaded as ansur_df. The fitted model will be used in the next exercise. Since we are not using the principal components themselves there is no need to transform the data, instead, it is sufficient to fit pca to the data.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Create the scaler.\n",
    "Standardize the data.\n",
    "Create the PCA() instance.\n",
    "Fit it to the standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(ansur_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA explained variance\n",
    "You'll be inspecting the variance explained by the different principal components of the pca instance you created in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the explained variance ratio per component\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Print the cumulative sum of the explained variance ratio\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the components\n",
    "You'll apply PCA to the numeric features of the Pokemon dataset, poke_df, using a pipeline to combine the feature scaling and PCA in one go. You'll then interpret the meanings of the first two components.\n",
    "\n",
    "All relevant packages and classes have been pre-loaded for you (Pipeline(), StandardScaler(), PCA())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "        \t\t ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit it to the dataset and extract the component vectors\n",
    "pipe.fit(poke_df)\n",
    "vectors = pipe['reducer'].components_.round(2)\n",
    "\n",
    "# Print feature effects\n",
    "print('PC 1 effects = ' + str(dict(zip(poke_df.columns, vectors[0]))))\n",
    "print('PC 2 effects = ' + str(dict(zip(poke_df.columns, vectors[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for feature exploration\n",
    "You'll use the PCA pipeline you've built in the previous exercise to visually explore how some categorical features relate to the variance in poke_df. These categorical features (Type & Legendary) can be found in a separate DataFrame poke_cat_df.\n",
    "\n",
    "All relevant packages and classes have been pre-loaded for you (Pipeline(), StandardScaler(), PCA())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit the pipeline to poke_df and transform the data\n",
    "pc = pipe.fit_transform(poke_df)\n",
    "\n",
    "# Add the 2 components to poke_cat_df\n",
    "poke_cat_df['PC 1'] = pc[:, 0]\n",
    "poke_cat_df['PC 2'] = pc[:, 1]\n",
    "\n",
    "# Use the Type feature to color the PC 1 vs. PC 2 scatterplot\n",
    "sns.scatterplot(data=poke_cat_df, \n",
    "                x='PC 1', y='PC 2', hue=\"Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA in a model pipeline\n",
    "We just saw that legendary Pokemon tend to have higher stats overall. Let's see if we can add a classifier to our pipeline that detects legendary versus non-legendary Pokemon based on the principal components.\n",
    "\n",
    "The data has been pre-loaded for you and split into training and tests datasets: X_train, X_test, y_train, y_test.\n",
    "\n",
    "Same goes for all relevant packages and classes(Pipeline(), StandardScaler(), PCA(), RandomForestClassifier())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reducer', PCA(n_components=3)),\n",
    "        ('classifier', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe.score(X_test, y_test)\n",
    "\n",
    "# Prints the explained variance ratio and accuracy\n",
    "print(pipe['reducer'].explained_variance_ratio_)\n",
    "print(f'{accuracy:.1%} test set accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the proportion of variance to keep\n",
    "You'll let PCA determine the number of components to calculate based on an explained variance threshold that you decide.\n",
    "\n",
    "You'll work on the numeric ANSUR female dataset pre-loaded as ansur_df.\n",
    "\n",
    "All relevant packages and classes have been pre-loaded too (Pipeline(), StandardScaler(), PCA())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let PCA select 90% of the variance\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "        \t\t ('reducer', PCA(n_components=0.9))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "pipe.fit(ansur_df)\n",
    "\n",
    "print(f'{len(pipe[\"reducer\"].components_)} components selected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the number of components\n",
    "You'll now make a more informed decision on the number of principal components to reduce your data to using the \"elbow in the plot\" technique. One last time, you'll work on the numeric ANSUR female dataset pre-loaded as ansur_df.\n",
    "\n",
    "All relevant packages and classes have been pre-loaded for you (Pipeline(), StandardScaler(), PCA())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline a scaler and pca selecting 10 components\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "        \t\t ('reducer', PCA(n_components=10))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "pipe.fit(ansur_df)\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "plt.plot(pipe['reducer'].explained_variance_ratio_)\n",
    "\n",
    "plt.xlabel('Principal component index')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for image compression\n",
    "You'll reduce the size of 16 images with hand written digits (MNIST dataset) using PCA.\n",
    "\n",
    "The samples are 28 by 28 pixel gray scale images that have been flattened to arrays with 784 elements each (28 x 28 = 784) and added to the 2D numpy array X_test. Each of the 784 pixels has a value between 0 and 255 and can be regarded as a feature.\n",
    "\n",
    "A pipeline with a scaler and PCA model to select 78 components has been pre-loaded for you as pipe. This pipeline has already been fitted to the entire MNIST dataset except for the 16 samples in X_test.\n",
    "\n",
    "Finally, a function plot_digits has been created for you that will plot 16 images in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the input data to principal components\n",
    "pc = pipe.transform(X_test)\n",
    "\n",
    "# Inverse transform the components to original feature space\n",
    "X_rebuilt = pipe.inverse_transform(pc)\n",
    "\n",
    "# Plot the reconstructed data\n",
    "plot_digits(X_rebuilt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
