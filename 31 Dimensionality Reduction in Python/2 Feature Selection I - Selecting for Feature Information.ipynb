{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course - Dimensionality Reduction in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Feature Selection I - Selecting for Feature Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - test split\n",
    "In this chapter, you will keep working with the ANSUR dataset. Before you can build a model on your dataset, you should first decide on which feature you want to predict. In this case, you're trying to predict gender.\n",
    "\n",
    "You need to extract the column holding this feature from the dataset and then split the data into a training and test set. The training set will be used to train the model and the test set will be used to check its performance on unseen data.\n",
    "\n",
    "ansur_df has been pre-loaded for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the train_test_split function from sklearn.model_selection.\n",
    "Assign the 'Gender' column to y.\n",
    "Remove the 'Gender' column from the DataFrame and assign the result to X.\n",
    "Set the test size to 30% to perform a 70% train and 30% test data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select the Gender column as the feature to be predicted (y)\n",
    "y = ansur_df['Gender']\n",
    "\n",
    "# Remove the Gender column to create the training data\n",
    "X = ansur_df.drop('Gender',axis=1)\n",
    "\n",
    "# Perform a 70% train and 30% test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(f\"{X_test.shape[0]} rows in test set vs. {X_train.shape[0]} in training set, {X_test.shape[1]} Features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and testing the model\n",
    "In the previous exercise, you split the dataset into X_train, X_test, y_train, and y_test. These datasets have been pre-loaded for you. You'll now create a support vector machine classifier model (SVC()) and fit that to the training data. You'll then calculate the accuracy on both the test and training set to detect overfitting.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Import SVC from sklearn.svm and accuracy_score from sklearn.metrics\n",
    "Create an instance of the Support Vector Classification class (SVC()).\n",
    "Fit the model to the training data.\n",
    "Calculate accuracy scores on both train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SVC from sklearn.svm and accuracy_score from sklearn.metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an instance of the Support Vector Classification class\n",
    "svc = SVC()\n",
    "\n",
    "# Fit the model to the training data\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print(f\"{accuracy_test:.1%} accuracy on test set vs. {accuracy_train:.1%} on training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy after dimensionality reduction\n",
    "You'll reduce the overfit with the help of dimensionality reduction. In this case, you'll apply a rather drastic form of dimensionality reduction by only selecting a single column that has some good information to distinguish between genders. You'll repeat the train-test split, model fit and prediction steps to compare the accuracy on test versus training data.\n",
    "\n",
    "All relevant packages and y have been pre-loaded.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Select just the neck circumference ('neckcircumferencebase') column from ansur_df.\n",
    "Split the data, instantiate a classifier and fit the data. This has been done for you.\n",
    "Once again calculate the accuracy scores on both training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign just the 'neckcircumferencebase' column from ansur_df to X\n",
    "X = ansur_df[['neckcircumferencebase']]\n",
    "\n",
    "# Split the data, instantiate a classifier and fit the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print(f\"{accuracy_test:.1%} accuracy on test set vs. {accuracy_train:.1%} on training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features with missing values or little variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features with low variance\n",
    "In the previous exercise you established that 0.001 is a good threshold to filter out low variance features in head_df after normalization. Now use the VarianceThreshold feature selector to remove these features.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Create the variance threshold selector with a threshold of 0.001.\n",
    "Normalize the head_df DataFrame by dividing it by its mean values and fit the selector.\n",
    "Create a boolean mask from the selector using .get_support().\n",
    "Create a reduced DataFrame by passing the mask to the .loc[] method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create a VarianceThreshold feature selector\n",
    "sel = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "# Fit the selector to normalized head_df\n",
    "sel.fit(head_df /head_df.mean())\n",
    "\n",
    "# Create a boolean mask\n",
    "mask = sel.get_support()\n",
    "\n",
    "# Apply the mask to create a reduced DataFrame\n",
    "reduced_df = head_df.loc[:, mask]\n",
    "\n",
    "print(f\"Dimensionality reduced from {head_df.shape[1]} to {reduced_df.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing features with many missing values\n",
    "You'll apply feature selection on the Boston Public Schools dataset which has been pre-loaded as school_df. Calculate the missing value ratio per feature and then create a mask to remove features with many missing values.\n",
    "\n",
    "Instructions \n",
    "\n",
    "Create a boolean mask on whether each feature has less than 50% missing values.\n",
    "Apply the mask to school_df to select columns without many missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask on whether each feature less than 50% missing values.\n",
    "mask = school_df.isna().sum() / len(school_df) < 0.5\n",
    "\n",
    "# Create a reduced dataset by applying the mask\n",
    "reduced_df =school_df.loc[:,mask]\n",
    "\n",
    "print(school_df.shape)\n",
    "print(reduced_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the correlation matrix\n",
    "Reading the correlation matrix of ansur_df in its raw, numeric format doesn't allow us to get a quick overview. Let's improve this by removing redundant values and visualizing the matrix using seaborn.\n",
    "\n",
    "Seaborn has been pre-loaded as sns, matplotlib.pyplot as plt, NumPy as np and pandas as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create the correlation matrix.\n",
    "Visualize it using Seaborn's heatmap function.\"\"\"\n",
    "\n",
    "# Create the correlation matrix\n",
    "corr = ansur_df.corr()\n",
    "\n",
    "# Draw a heatmap of the correlation matrix\n",
    "sns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Create a boolean mask for the upper triangle of the plot.\"\"\"\n",
    "\n",
    "# Create the correlation matrix\n",
    "corr = ansur_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "print(mask)\n",
    "\n",
    "\n",
    "\"\"\"Add the mask to the heatmap.\"\"\"\n",
    "\n",
    "# Add the mask to the heatmap\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing highly correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out highly correlated features\n",
    "You're going to automate the removal of highly correlated features in the numeric ANSUR dataset. You'll calculate the correlation matrix and filter out columns that have a correlation coefficient of more than 0.95 or less than -0.95.\n",
    "\n",
    "Since each correlation coefficient occurs twice in the matrix (correlation of A to B equals correlation of B to A) you'll want to ignore half of the correlation matrix so that only one of the two correlated features is removed. Use a mask trick for this purpose.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Calculate the correlation matrix of ansur_df and take the absolute value of this matrix.\n",
    "Create a boolean mask with True values in the upper right triangle and apply it to the correlation matrix.\n",
    "Set the correlation coefficient threshold to 0.95.\n",
    "Drop all the columns listed in to_drop from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix and take the absolute value\n",
    "corr_df = ansur_df.corr().abs()\n",
    "\n",
    "# Create a True/False mask and apply it\n",
    "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
    "tri_df = corr_df.mask(mask)\n",
    "\n",
    "# List column names of highly correlated features (r > 0.95)\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] >  0.95)]\n",
    "\n",
    "# Drop the features in the to_drop list\n",
    "reduced_df = ansur_df.drop(to_drop, axis=1)\n",
    "\n",
    "print(f\"The reduced_df DataFrame has {reduced_df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuclear energy and pool drownings\n",
    "The dataset that has been pre-loaded for you as weird_df contains actual data provided by the US Centers for Disease Control & Prevention and Department of Energy.\n",
    "\n",
    "Let's see if we can find a pattern.\n",
    "\n",
    "Seaborn has been pre-loaded as sns and matplotlib.pyplot as plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five lines of weird_df\n",
    "print(weird_df.head())\n",
    "\n",
    "# Put nuclear energy production on the x-axis and the number of pool drownings on the y-axis\n",
    "sns.scatterplot(x=\"pool_drownings\", y=\"nuclear_energy\", data=weird_df)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
