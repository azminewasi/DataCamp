{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course -Cluster Analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Clustering in Real World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of k-means clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering: first exercise\n",
    "This exercise will familiarize you with the usage of k-means clustering on a dataset. Let us use the Comic Con dataset and check how k-means clustering works on it.\n",
    "\n",
    "Recall the two steps of k-means clustering:\n",
    "\n",
    "Define cluster centers through kmeans() function. It has two required arguments: observations and number of clusters.\n",
    "Assign cluster labels through the vq() function. It has two required arguments: observations and cluster centers.\n",
    "The data is stored in a pandas DataFrame, comic_con. x_scaled and y_scaled are the column names of the standardized X and Y coordinates of people at a given point in time.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Import kmeans and vq functions in SciPy.\n",
    "Generate cluster centers using the kmeans() function with two clusters.\n",
    "Create cluster labels using these cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kmeans and vq functions\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "# Generate cluster centers\n",
    "cluster_centers, distortion = kmeans(comic_con[[\"x_scaled\",\"y_scaled\"]],2)\n",
    "\n",
    "# Assign cluster labels\n",
    "comic_con['cluster_labels'], distortion_list = vq(comic_con[[\"x_scaled\",\"y_scaled\"]],cluster_centers)\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract RGB values from image\n",
    "There are broadly three steps to find the dominant colors in an image:\n",
    "\n",
    "Extract RGB values into three lists.\n",
    "Perform k-means clustering on scaled RGB values.\n",
    "Display the colors of cluster centers.\n",
    "To extract RGB values, we use the imread() function of the image class of matplotlib. Empty lists, r, g and b have been initialized.\n",
    "\n",
    "For the purpose of finding dominant colors, we will be using the following image.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Import image class of matplotlib.\n",
    "Read the image using the imread() function and print the dimensions of the resultant matrix.\n",
    "Store the values for the three colors from all pixels in lists r, g and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import image class of matplotlib\n",
    "from matplotlib import image as img\n",
    "\n",
    "# Read batman image and print dimensions\n",
    "batman_image = img.imread('batman.jpg')\n",
    "print(batman_image.shape)\n",
    "\n",
    "# Store RGB values of all pixels in lists r, g and b\n",
    "for row in batman_image:\n",
    "    for temp_r, temp_g, temp_b in row:\n",
    "        r.append(temp_r)\n",
    "        g.append(temp_g)\n",
    "        b.append(temp_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many dominant colors?\n",
    "We have loaded the following image using the imread() function of the image class of matplotlib.\n",
    "\n",
    "The RGB values are stored in a DataFrame, batman_df. The RGB values have been standardized used the whiten() function, stored in columns, scaled_red, scaled_blue and scaled_green.\n",
    "\n",
    "Construct an elbow plot with the DataFrame. How many dominant colors are present?\n",
    "\n",
    "Instructions\n",
    "\n",
    "Create a list of distortions based on each value in num_clusters by running the kmeans() function.\n",
    "Create a DataFrame elbow_plot with the lists: num_clusters and distortions.\n",
    "Plot the data with seaborn's .lineplot() method with num_clusters on the x-axis and distortions on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "num_clusters = range(1, 7)\n",
    "\n",
    "# Create a list of distortions from the kmeans function\n",
    "for i in num_clusters:\n",
    "    cluster_centers, distortion = kmeans(batman_df[['scaled_red', 'scaled_blue', 'scaled_green']], i)\n",
    "    distortions.append(distortion)\n",
    "\n",
    "# Create a DataFrame with two lists, num_clusters and distortions\n",
    "elbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})\n",
    "\n",
    "# Create a line plot of num_clusters and distortions\n",
    "sns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)\n",
    "plt.xticks(num_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display dominant colors\n",
    "We have loaded the following image using the imread() function of the image class of matplotlib.\n",
    "To display the dominant colors, convert the colors of the cluster centers to their raw values and then converted them to the range of 0-1, using the following formula: converted_pixel = standardized_pixel * pixel_std / 255\n",
    "\n",
    "The RGB values are stored in a DataFrame, batman_df. The scaled RGB values are stored in columns, scaled_red, scaled_blue and scaled_green. The cluster centers are stored in the variable cluster_centers, which were generated using the kmeans() function with three clusters.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Get standard deviations of each color from the DataFrame and store it in r_std, g_std, b_std.\n",
    "For each cluster center, convert the standardized RGB values to scaled values in the range of 0-1.\n",
    "Display the colors of the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get standard deviations of each color\n",
    "r_std, g_std, b_std = batman_df[['red', 'green', 'blue']].std()\n",
    "\n",
    "for cluster_center in cluster_centers:\n",
    "    scaled_r, scaled_g, scaled_b = cluster_center\n",
    "    # Convert each standardized value to scaled value\n",
    "    colors.append((\n",
    "        scaled_r * r_std / 255,\n",
    "        scaled_g * g_std / 255,\n",
    "        scaled_b * b_std / 255\n",
    "    ))\n",
    "\n",
    "# Display colors of cluster centers\n",
    "plt.imshow([colors])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF of movie plots\n",
    "Let us use the plots of randomly selected movies to perform document clustering on. Before performing clustering on documents, they need to be cleaned of any unwanted noise (such as special characters and stop words) and converted into a sparse matrix through TF-IDF of the documents.\n",
    "\n",
    "Use the TfidfVectorizer class to perform the TF-IDF of movie plots stored in the list plots. The remove_noise() function is available to use as a tokenizer in the TfidfVectorizer class. The .fit_transform() method fits the data into the TfidfVectorizer objects and then generates the TF-IDF sparse matrix.\n",
    "\n",
    "Note: It takes a few seconds to run the .fit_transform() method.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Import TfidfVectorizer class from sklearn.\n",
    "Initialize the TfidfVectorizer class with minimum and maximum frequencies of 0.1 and 0.75, and 50 maximum features.\n",
    "Use the fit_transform() method on the initialized TfidfVectorizer class with the list plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer class from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=0.1,max_df=0.75,max_features=50,tokenizer=remove_noise)\n",
    "\n",
    "# Use the .fit_transform() method on the list plots\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top terms in movie clusters\n",
    "Now that you have created a sparse matrix, generate cluster centers and print the top three terms in each cluster. Use the .todense() method to convert the sparse matrix, tfidf_matrix to a normal matrix for the kmeans() function to process. Then, use the .get_feature_names() method to get a list of terms in the tfidf_vectorizer object. The zip() function in Python joins two lists.\n",
    "\n",
    "The tfidf_vectorizer object and sparse matrix, tfidf_matrix, from the previous have been retained in this exercise. kmeans has been imported from SciPy.\n",
    "\n",
    "With a higher number of data points, the clusters formed would be defined more clearly. However, this requires some computational power, making it difficult to accomplish in an exercise here.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Generate cluster centers through the kmeans() function.\n",
    "Generate a list of terms from the tfidf_vectorizer object.\n",
    "Print top 3 terms of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2\n",
    "\n",
    "# Generate cluster centers through the kmeans function\n",
    "cluster_centers, distortion = kmeans(tfidf_matrix.todense(),num_clusters) \n",
    "\n",
    "# Generate terms from the tfidf_vectorizer object\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    # Sort the terms and print top 3 terms\n",
    "    center_terms = dict(zip(terms,list(cluster_centers[i])))\n",
    "    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)\n",
    "    print(sorted_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic checks on clusters\n",
    "In the FIFA 18 dataset, we have concentrated on defenders in previous exercises. Let us try to focus on attacking attributes of a player. Pace (pac), Dribbling (dri) and Shooting (sho) are features that are present in attack minded players. In this exercise, k-means clustering has already been applied on the data using the scaled values of these three attributes. Try some basic checks on the clusters so formed.\n",
    "\n",
    "The data is stored in a pandas DataFrame, fifa. The scaled column names are present in a list scaled_features. The cluster labels are stored in the cluster_labels column. Recall the .count() and .mean() methods in pandas help you find the number of observations and mean of observations in a DataFrame.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Print the size of the clusters by grouping the column cluster_labels.\n",
    "Print the mean values of the wages of the players in each cluster. eur_wage is the column name that stores the wages of a player in Euros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the size of the clusters\n",
    "print(fifa.groupby(\"cluster_labels\")['ID'].count())\n",
    "\n",
    "# Print the mean value of wages in each cluster\n",
    "print(fifa.groupby(\"cluster_labels\")['eur_wage'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIFA 18: what makes a complete player?\n",
    "The overall level of a player in FIFA 18 is defined by six characteristics: pace (pac), shooting (sho), passing (pas), dribbling (dri), defending (def), physical (phy).\n",
    "\n",
    "Here is a sample card:\n",
    "\n",
    "Eden Hazard Player Card\n",
    "\n",
    "In this exercise, you will use all six characteristics to create clusters. The data for this exercise is stored in a pandas DataFrame, fifa. features is the list of these column names and scaled_features is the list of columns which contains their scaled values. The following have been pre-loaded: kmeans, vq from scipy.cluster.vq, matplotlib.pyplot as plt, seaborn as sns.\n",
    "\n",
    "Before you start the exercise, you may wish to explore scaled_features in the IPython Shell to check out the list of six scaled columns names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create centroids with kmeans for 2 clusters\n",
    "cluster_centers,_ = kmeans(fifa[scaled_features], 2)\n",
    "\n",
    "# Assign cluster labels and print cluster centers\n",
    "fifa['cluster_labels'], _ = vq(fifa[scaled_features], cluster_centers)\n",
    "print(fifa.groupby('cluster_labels')[scaled_features].mean())\n",
    "\n",
    "# Plot cluster centers to visualize clusters\n",
    "fifa.groupby('cluster_labels')[scaled_features].mean().plot(legend=True, kind='bar')\n",
    "plt.show()\n",
    "\n",
    "# Get the name column of first 5 players in each cluster\n",
    "for cluster in fifa['cluster_labels'].unique():\n",
    "    print(cluster, fifa[fifa['cluster_labels'] == cluster][\"name\"].values[:5])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
