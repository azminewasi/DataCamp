{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course - Preprocessing for Machine Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Standardizing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling without normalizing\n",
    "Let's take a look at what might happen to your model's accuracy if you try to model data without doing some sort of standardization first. Here we have a subset of the wine dataset. One of the columns, Proline, has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which you'll learn about in the next section.\n",
    "\n",
    "The scikit-learn model training process should be familiar to you at this point, so we won't go too in-depth with it. You already have a k-nearest neighbors model available (knn) as well as the X and y sets you need to fit and score on.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Split up the X and y sets into training and test sets using train_test_split().\n",
    "Use the knn model's fit() method on the X_train data and y_train labels, to fit the model to the data.\n",
    "Print out the knn model's score() on the X_test data and y_test labels to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log normalization in Python\n",
    "Now that we know that the Proline column in our wine dataset has a large amount of variance, let's log normalize it.\n",
    "\n",
    "Numpy has been imported as np in your workspace.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Print out the variance of the Proline column for reference.\n",
    "Use the np.log() function on the Proline column to create a new, log-normalized column named Proline_log.\n",
    "Print out the variance of the Proline_log column to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the variance of the Proline column\n",
    "print(wine.Proline.var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine[\"Proline_log\"] = np.log(wine[\"Proline\"])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(wine.Proline_log.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling data for feature comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data - standardizing columns\n",
    "Since we know that the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset are all on different scales, let's standardize them in a way that allows for use in a linear model.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Import StandardScaler from sklearn.preprocessing.\n",
    "Create the StandardScaler() method and store in a variable named ss.\n",
    "Create a subset of the wine DataFrame of the Ash, Alcalinity of ash, and Magnesium columns, store in a variable named wine_subset.\n",
    "Apply the ss.fit_transform method to the wine_subset DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Take a subset of the DataFrame you want to scale \n",
    "wine_subset = wine[[\"Ash\",\"Alcalinity of ash\",\"Magnesium\"]]\n",
    "\n",
    "# Apply the scaler to the DataFrame subset\n",
    "wine_subset_scaled = ss.fit_transform(wine_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized data and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN on non-scaled data\n",
    "Let's first take a look at the accuracy of a K-nearest neighbors model on the wine dataset without standardizing the data. The knn model as well as the X and y data and labels sets have been created already. Most of this process of creating models in scikit-learn should look familiar to you.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Split the dataset into training and test sets using train_test_split().\n",
    "Use the knn model's fit() method on the X_train data and y_train labels, to fit the model to the data.\n",
    "Print out the knn model's score() on the X_test data and y_test labels to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN on scaled data\n",
    "The accuracy score on the unscaled wine dataset was decent, but we can likely do better if we scale the dataset. The process is mostly the same as the previous exercise, with the added step of scaling the data. Once again, the knn model as well as the X and y data and labels set have already been created for you.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Create the StandardScaler() method, stored in a variable named ss.\n",
    "Apply the ss.fit_transform method to the X dataset.\n",
    "Use the knn model's fit() method on the X_train data and y_train labels, to fit the model to the data.\n",
    "Print out the knn model's score() on the X_test data and y_test labels to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scaling method.\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Apply the scaling method to the dataset used for modeling.\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
